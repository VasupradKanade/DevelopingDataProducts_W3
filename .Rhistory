library(swirl)
swirl()
.64
.64
mypdf
integrate(mypdf,0,1.6)
1.414
1.414214
.997*.001
(1-.985)*(1-.001)
000997/(.000997+.014985)
.000997/(.000997+.014985)
3.5
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
3.5
integrate(myfunc,0,2)
spop
mean(spop)
allsam
apply(allsam,1,mean)
mean(smeans)
quit()
library(swirl)
swirl()
swirl()
dice_sqr
ex2_fair <- sum(dice_fair * dice_sqr)
ex2_fair-3.5^2
sum(dice_high * dice_sqr)-edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
2/sqrt(n)
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
0.94208
pbinom(2,size=5,prob=.8,lower.tail=FALSE)
qnorm(.10)
2
-2.33
1.96
0
qnorm(.975,mean=3,sd=2)
6.92
pnorm(1200,mean=1020,sd=50,lower.tail=FALSE)
pnorm((1200-1020)/50,lower.tail=FALSE)
qnorm(.75,mean=1020,sd=50)
.53
.53
ppois(3,2.5 * 4)
pbinom(5,1000,.01)
ppois(5,1000*.01)
coinPlot(10)
coinPlot(10000)
qnorm(.95)
.6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100)
binom.test(60,100)$conf.int
mywald(.2)
ACCompar(20)
lamb <- 5/94.32
lamb +c(-1,1)*qnorm(.975)*sqrt(lamb/94.32)
poisson.test(5,94.32)$conf
myplot(2)
myplot(20)
myplot(2)
myplot2(2)
qt(.975,2)
myplot2(20)
sleep
range(g1)
range(g2)
difference <- g2-g1
mean(difference)
s <- sd(difference)
mn + c(-1,1)*qt(.975,9)*s/sqrt(10)
t.test(difference)$conf.int
sp <- 7*15.34^2 + 20*18.23^2
ns <- 8+21-2
sp <- sqrt(sp/ns)
132.86-127.44+c(-1,1)*qt(.975,ns)*sp*sqrt(1/8+1/21)
sp <- sqrt((9*var(g1)+9*var(g2))/18)
md + c(-1,1)*qt(.975,18)*sp*sqrt(1/5)
t.test(g2,g1,paired=FALSE,var.equal=TRUE)$conf
t.test(g2,g1,paired=TRUE)$conf
num <- (15.34^2/8 + 18.23^2/21)^2
den <- 15.34^4/8^2/7 + 18.23^4/21^2/20
mydf <- num/den
132.86-127.44 +c(-1,1)*qt(.975,mydf)*sqrt(15.34^2/8 + 18.23^2/21)
1
2
.8
15
qtqt(.95,15)
qt(.95,15)
dim(fs)
t.test(fs$sheight-fs$fheight)
11.7885 * sd(fs$sheight-fs$fheight)/sqrt(1078)
mybin
8
pt(2.5, 15, lower.tail=FALSE)
qnorm(.95)
qnorm(.99)
pnorm(2)
mybin
pnorm(2, lower.tail=FALSE)
mybin
pbinom(6,size=8,prob=.5,lower.tail=FALSE)
pbinom(7,size=8,prob=.5,lower.tail=TRUE)
ppois(9,5,lower.tail=FALSE)
quit()
library(swirl)
swirl()
table(p.adjust(pValues2,method=\"bonferroni\") < 0.05, trueStatus)
"table(p.adjust(pValues2,method=\"bonferroni\") < 0.05, trueStatus)"
table(p.adjust(pValues2,method="bonferroni") < 0.05, trueStatus)
table(p.adjust(pValues2,method="BH") < 0.05, trueStatus)
3.5
print (g2)
head(sh)
n
nh
median(resampledMedians)
median(sh)
sam <- sample(fh,nh*B,replace=TRUE)
resam <- matrix(sam,B,nh)
meds <- apply(resam,1,median)
median(meds)-median(fh)
sd(meds)
sd(resampledMedians)
quantile(resampledMedians,c(.025,.975))
quantile(meds,c(.025,.975))
(InsectSprays)
dim(InsectSprays)
names(InsectSprays)
range(Bdata$count)
range(Cdata$count)
BCcounts
group
testStat
obs <- testStat(BCcounts,group)
obs
mean(Bdata$count)-mean(Cdata$count)
sample(group)
perms <- sapply(1 : 10000, function(i) testStat(BCcounts, sample(group)))
mean(perms>obs)
testStat(DEcounts,group)
perms <- sapply(1 : 10000, function(i) testStat(DEcounts, sample(group)))
quit()
library(swirl)
swirl()
swirl()
install_from_swirl("Regression Models")
swirl()
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child ~ parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals,galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs-rhs
all.equal(lhs,rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
swirl()
cor(gpa_nor,gch_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
quit()
library(swirl)
swirl()
fit <- lm(child ~ parent, galton)
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
1-sRes/sTot
summary(fit)$r.squared
cor(galton$parent,galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent - 1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
head(trees)
fit <- lm(Volume ~ . - 1, trees)
'trees2 <- eliminate("Girth", trees)'
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ . - 1, trees2)
lapply(list(fit, fit2), coef)
swirl()
all <- lm(Fertility ~ ., swiss)
summary(all)
summary(lm(Fertility ~ Agriculture, swiss))
cor(swiss$Examination,swiss$Education)
cor(swiss$Agriculture,swiss$Education)
makelms()
ec <- swiss$Examination+swiss$Catholic
efit <- lm(Fertility ~ . + ec, swiss)
efit <- lm(Fertility ~ . + ec, swiss)
all$coefficients - efit$coefficients
quit()
library(swirl)
swirl()
6
dim(InsectSprays)
head(InsectSprays,15)
sA
summary(InsectSprays[,2])
sapply(InsectSprays,class)
fit <- lm(count ~ spray, InsectSprays)
summary(fit)$coef
est <- summary(fit)$coef[,1]
mean(sA)
mean(sB)
nfit <- lm(count ~ spray - 1, InsectSprays)
summary(nfit)$coef
spray2 <- relevel(InsectSprays$spray,"C")
fit2 <- lm(count ~ spray2, InsectSprays)
summary(fit2)$coef
mean(sC)
(fit$coef[2]-fit$coef[3])/1.6011
dim(hunger)
948
names(hunger)
fit <- lm(hunger$Numeric ~ hunger$Year)
summary(fit)$coef
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
summary(lmBoth)
lmInter <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Year * hunger$Sex)
summary(lmInter)
fit <- lm(y ~ x, out2)
plot(fit, which=1)
fitno <- lm(y ~ x, out2[-1, ])
plot(fitno, which=1)
coef(fit)-coef(fitno)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
head(rstudent(fit))
dy <- predict(fitno, out2)-predict(fit, out2)
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
quit()
library(swirl)
swirl()
swirl()
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ ., swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . -Examination, swiss)
vif(mdl2)
swirl()
quit()
##Vasuprad.Kanade@accenture.com
## Course 7-Regression: Quiz 4
library(MASS)
data(shuttle)
head(shuttle)
#Checking out the data
unique(shuttle$use)
unique(shuttle$wind)
#Creating 0,1 variable for auto/noauto factor
shuttle$use <- as.numeric(shuttle$use == "auto")
#generating model
mdl <- glm(factor(use)~factor(wind)-1,binomial,data = shuttle)
exp(mdl$coef[1])/exp(mdl$coef[2])
#Checking out the factor levels
unique(shuttle$magn)
mdl2 <- glm(factor(use)~factor(wind)+factor(magn)-1,binomial,data = shuttle)
summary(mdl2)
exp(mdl2$coef[1])/exp(mdl2$coef[2])
mdl3 <- glm(1-use~factor(wind)-1,binomial,data = shuttle)
mdl3$coef
data("InsectSprays")
mdl4 <- glm(count~spray-1,poisson,data = InsectSprays)
exp(mdl4$coef[1])/exp(mdl4$coef[2])
mdl5 <- glm(count~spray,poisson,offset = log(count+1),data = InsectSprays)
mdl6 <- glm(count~spray,poisson,offset = log(10)+log(count+1),data = InsectSprays)
mdl6$coef
mdl5$coef
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
k<-c(0)
split<-sapply(k,function(k) (x>k)*(x-k))
xmat<-cbind(1,x,split)
mdl7 <- lm(y~xmat-1)
yhat<-predict(mdl7)
mdl7$coef
mdl7$coef[3]+mdl7$coef[2]
plot(x,y)
lines(x,yhat, col= "red", lwd =2)
library(AppliedPredictiveModeling)
install.packages("C:/Users/vasuprad.kanade/Downloads/AppliedPredictiveModeling_1.1-6.zip", repos = NULL, type = "win.binary")
library(AppliedPredictiveModeling)
install.packages("installr") # install
installr::updateR() # updating R.
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
q()
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
# setting the overall seed for reproduceability
set.seed(1234)
# After saving both data sets into my working directory
setwd("~/Vasuprad/Accenture/Official/Trainings/Coursera/Data Science/Assignments/Course 8-Mahchine Learning")
# Some missing values are coded as string "#DIV/0!" or "" or "NA" - these will be changed to NA.
# Loading the training data set into my R session replacing all missing with "NA"
trainingset <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
# Loading the testing data set
testingset <- read.csv('pml-testing.csv', na.strings=c("NA","#DIV/0!", ""))
# Check dimensions for number of variables and number of observations
dim(trainingset)
dim(testingset)
# Delete columns with all missing values
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]
# Some variables are irrelevant to our current project: user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7). We can delete these variables.
trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]
# and have a look at our new datasets:
dim(trainingset)
dim(testingset)
head(trainingset)
head(testingset)
subsamples <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
subTraining <- trainingset[subsamples, ]
subTesting <- trainingset[-subsamples, ]
dim(subTraining)
dim(subTesting)
head(subTraining)
head(subTesting)
model1 <- rpart(classe ~ ., data=subTraining, method="class")
# Predicting:
prediction1 <- predict(model1, subTesting, type = "class")
# Plot of the Decision Tree
rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
# Test results on our subTesting data set:
confusionMatrix(prediction1, subTesting$classe)
model2 <- randomForest(classe ~. , data=subTraining, method="class")
# Predicting:
prediction2 <- predict(model2, subTesting, type = "class")
# Test results on subTesting data set:
confusionMatrix(prediction2, subTesting$classe)
predictfinal <- predict(model2, testingset, type="class")
predictfinal
corMatrix <- cor(trainingset[, -54])
library(corrplot)
install.packages("corrplot")
library(corrplot)
corMatrix <- cor(trainingset[, -54])
corMatrix <- cor(trainingset[, -54])
corMatrix <- cor(trainingset[, -53])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
fancyRpartPlot(model1)
rpart.fancyRpartPlot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
fancyRpartPlot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
library(rpart)
library(rpart.plot)
fancyRpartPlot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
fancyRpartPlot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
install.packages("fancyRpartPlot")
rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
library(knitr)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
install.packages("rattle")
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM  <- train(classe ~ ., data=subTraining, method = "gbm",
trControl = controlGBM, verbose = FALSE)
modFitGBM$finalModel
# prediction on Test dataset
predictGBM <- predict(model3, newdata=TestSet)
model3  <- train(classe ~ ., data=subTraining, method = "gbm",
trControl = controlGBM, verbose = FALSE)
model3$finalModel
# prediction on Test dataset
predictGBM <- predict(model3, newdata=TestSet)
predictGBM <- predict(model3, newdata=subTesting)
confMatGBM <- confusionMatrix(predictGBM, TestSet$classe)
confMatGBM <- confusionMatrix(predictGBM, subTesting$classe)
confMatGBM
# plot matrix results
plot(confMatGBM$table, col = confMatGBM$byClass,
main = paste("GBM - Accuracy =", round(confMatGBM$overall['Accuracy'], 4)))
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
library(corrplot)
# setting the overall seed for reproduceability
set.seed(1234)
knitr::opts_chunk$set(echo = TRUE)
##Vasuprad.Kanade@accenture.com
## Course 9-Developing Data Products - Assignment 1
library(leaflet)
my_map<- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lat=18.520432, lng=73.855408, popup="Shaniwar Wada, Pune, India")
my_map
map1 <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lat=18.520432, lng=73.855408, popup="Shaniwar Wada, Pune, India")
map1
setwd("~/Vasuprad/Accenture/Official/Trainings/Coursera/Data Science/Assignments/Course 9-Data Products")
library(devtools)
library(slidify)
author('Week3Pres')
---
title       : Developing Data Products - Plot using Plotly
subtitle    : Course 9 - Week 3 - Assignment Presentation
author      : Vasuprad Kanade
job         :
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      #
widgets     : [bootstrap, quiz]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---
## Read-And-Delete
1. Edit YAML front matter
2. Write using R Markdown
3. Use an empty line followed by three dashes to separate slides!
--- .class #id
## Slide 2
This is my first Slide
slidify('index.Rmd')
slidify('Index.RMD')
slidify('Index.RMD')
slidify('Index.RMD')
# Chunk 1: Initiation
library(ggplot2)
library(plotly)
# Chunk 2
filename <- "iris.csv"
# Load the CSV file from the local directory
dataset <- read.csv(filename, header=FALSE)
# Set the column names in the dataset
colnames(dataset) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species")
# Chunk 3
p <- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species, colors = "Set1")
p
unlink('index_cache', recursive = TRUE)
rmarkdown::shiny_prerendered_clean('index.Rmd')
```{r Initiation, echo=FALSE, message=FALSE, eval=TRUE}
install.packages("webshot")
p <- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species, colors = "Set1")
setwd("./assets/img")
saveWidget(p, 'plott1.html') # Save widget html
setwd("../..")
saveWidget(p, 'plott1.html') # Save widget html
library(htmlwidgets)
library(knitr)
library(dplyr)
saveWidget(p, 'plott1.html') # Save widget html
cat('<pre><iframe src="./assets/img/plot1.html" width=100% height=350px allowtransparency="true"> </iframe></pre>')
library(cars)
plot(cars)
abline(lm(dist ~ speed, data = cars), col = "red")
slidify('Index.RMD')
slidify('Index.RMD')
unlink('index_cache', recursive = TRUE)
rmarkdown::shiny_prerendered_clean('index.Rmd')
setwd("~/Vasuprad/Accenture/Official/Trainings/Coursera/Data Science/Assignments/Course 9-Data Products/Week3Pres")
slidify('Index.RMD')
